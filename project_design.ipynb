{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25da3442",
   "metadata": {},
   "source": [
    "2395383\n",
    "\n",
    "<h1><i>'Spikk Looder'</i>: Reviving the Shetland Dialect with AI</h1>\n",
    "\n",
    "<h2>Concept</h2>\n",
    "\n",
    "<p>‘SpikkBot’ is an AI-powered chatbot designed to promote and preserve Shetland dialect through engaging, conversational interactions. It uses a curated dataset of Shetland speech and phrases to respond to user input in a natural, friendly way. Inspired by the value of local heritage and the importance of accessible cultural preservation, ‘SpikkBot’ aims to engage users of all ages in learning and using the dialect confidently and authentically. 'Spikk' translates to 'Speak'.\n",
    "\n",
    "The data is to be sourced from a mixture of places: dialect speaking locals, ‘experts’ in Shetland dialect heritage e.g. through pronunciation, idioms, and spellings, along with the data that is currently available from the online Shetland Dictionary, the Shetland Museum and Archives, and other research papers. This ensures that ‘Spikkbot’ reflects the richness and diversity of Shetland’s oral and written history. Unlike general purpose chatbots, ‘Spikkbot’ is trained to understand and generate dialect specific grammar, idioms, speech, and pronunciation, creating an educational and engaging experience for the user.\n",
    "\n",
    "Developing the bot, a human-centred design (HCD) approach was at the core of the design process. The project began by identifying a meaningful gap, which was the inaccessibility or inactivity of Shetland dialect in everyday life. In line with step one of HDC, the project is not to just create a novelly piece of technology but to develop a tool that is shaped by and is for the people it serves. Additionally, the tool aims to work in alignment with the Scottish AI Strategy 2021.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21459529",
   "metadata": {},
   "source": [
    "<h2>User Flow Example</h2>\n",
    "\n",
    "<p><b>Scenario:</b> A young Shetlander wants to learn a phrase that their deceased grandparent used regularly.\n",
    "\n",
    "<b>User types:</b> “What does ‘an den dae made tae’ mean?”\n",
    "\n",
    "<b>Spikkbot responds:</b> “It means ‘and then they made tea’. It is an anecdotal term that is often used to finish a story, to poke fun at the number of times Shetlanders drink tea, which is before, during and after everything. Would you like to hear a pronunciation?”\n",
    "\n",
    "The user clicks ‘yes’, and the audio plays in authentic dialect voice. Spikkbot then offers to them practice the phrase. The user agrees, repeats the phrase out loud, and receives tailored feedback on their pronunciation.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2da07",
   "metadata": {},
   "source": [
    "<h2>Human Role</h2>\n",
    "\n",
    "<p>Human involvement is key for the development of AI bots and is particularly critical for Spikkbot. The project is not just about encoding dialect but about combining human judgement and voice into the system. As this project is all about empowering and reviving an endangered dialect, it is essential to involve the community within its development. Therefore, locals and experts will be ultised in the creation of the datatset used to train the model.\n",
    "\n",
    " Taking from the HCD framework, the process emphasises:\n",
    "\n",
    "<ul>\n",
    "<li> Collaboration with diverse community members, as Shetland dialect can vary from place to place it is important to include varying fluency to inform the bot.</li>\n",
    "<li> Reflections on whether AI was the right solution for this issue. Other non-AI options could be considered, but a generative model offers an engaging and interactive use experience which aligns with goals of the project.</li>\n",
    "</ul>\n",
    "\n",
    "Throughout the development of the project, human oversight will be essential. The dataset is not only reviewed by humans for accuracy and tone but is also open to public feedback. Users can submit issues, suggested changes, or any challenges they faced while using the bot. This follows step five of HCD, which emphasises the importance of user agency in correcting or questioning AI systems.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2cd60e",
   "metadata": {},
   "source": [
    "<h2>AI Components</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde76042",
   "metadata": {},
   "source": [
    "<p>At the core of the technology is the Large Language Model (LLM). This is trained on Shetland dialect texts and audios and is aligned with standard spoken English. The aim is ultimately to understand, interpret, and generate Shetland dialect. This involves careful design of the AI components working together to create natural interactions between the technology and the user.\n",
    "\n",
    "The conversational aspect of the LLM will respond in dialect and will outline pronunciations of phrases and words. This is in comparison to a generic chatbot like GPT-4, where the model may not be fine-tuned to specific dialects. This function will allow ‘Spikkbott’ to understand dialect specific grammar and vocabulary, ensuring that responses are authentic through the integration of locals in training.\n",
    "\n",
    "The speech recognition component of the model is proposed to be built on OpenAI’s ‘Whisper’. Whisper is an automatic speech recognition system (ASR) trained on multilingual datasets to then help with recognising and translating languages. The framework can be ran using Python, by setting up the environment with the pip function like the following: </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41be849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-h2n62urk\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-h2n62urk\n",
      "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting more-itertools (from openai-whisper==20240930)\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting numba (from openai-whisper==20240930)\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from openai-whisper==20240930) (2.2.4)\n",
      "Collecting tiktoken (from openai-whisper==20240930)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (from openai-whisper==20240930) (2.6.0+cpu)\n",
      "Collecting tqdm (from openai-whisper==20240930)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting triton>=2 (from openai-whisper==20240930)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from triton>=2->openai-whisper==20240930) (76.0.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba->openai-whisper==20240930)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper==20240930)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch->openai-whisper==20240930) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch->openai-whisper==20240930) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch->openai-whisper==20240930) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
      "Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803708 sha256=960632cc90b7f0b127069c02d5fc1870c4ad45a957581d0c68bfc90f3c671338\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1grwraa6/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: triton, tqdm, regex, more-itertools, llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.44.0 more-itertools-10.6.0 numba-0.61.2 openai-whisper-20240930 regex-2024.11.6 tiktoken-0.9.0 tqdm-4.67.1 triton-3.3.0\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.6.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git \n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13713efb",
   "metadata": {},
   "source": [
    "<p>‘Spikkbot’ will also feature a Shetland accent voice, based on text-to-speech (TTS) technology. This may utilise the framework from Tacotron 2, a two-part neutral network that converts written speech into audio speech. This will allow Spikkbot to speak in authentic dialect, as it is trained based on high quality audio along with exact transcriptions. Tacotron 2 can be input into Python, by setting up the environment with the pip function like the following: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5aab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy librosa unidecode inflect librosa\n",
    "apt-get update\n",
    "apt-get install -y libsndfile1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803b5fe",
   "metadata": {},
   "source": [
    "<p>‘Spikkbot’ will then require integrating a dialogue management system to ensure consistent back and forth interactions between the user and the chatbot. It will help remember past questions and ensure conversation flows naturally and meaningful. For example, if a user mispronounces a word, the system might offer a second attempt or break the word down for them.\n",
    "\n",
    "Finally, ‘Spikkbot’ is required to have a learning tracker to support learning over time, and to create personalised learning plans. This aligns with the overall aim of the project, as it mainly about engagement and encouraging the use of AI applications in language preservation\n",
    "\n",
    "<b>User Input (Speech or Text) – ASR (if speech) - LLM – TTS (if audio) – Feedback Engine</b>\n",
    "\n",
    "To evaluate the accuracy of SpikkBot’s speech recognition component, Word Error Rate (WER) can be utilised. WER measures how closely the transcribed text matches a known reference transcript by calculating the number of substitutions, insertions, and deletions required to transform the model’s output into the correct text. This metric is valuable in a project like SpikkBot because dialects often contain unfamiliar words and pronounciations that can trip up generic models. By calculating the WER on a curated dataset of Shetland dialect recordings with accurate transcriptions, the machine can track improvements as the model is continuously fine-tuned. The machine aims to get a lower WER as that suggests better transcription performance, and helpts to identify which dialect words are commonly incorrect, which will help to guide future model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456393f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.1.8 (from jiwer)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n",
      "Successfully installed click-8.1.8 jiwer-3.1.0 rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e456593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 20.00%\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "# Reference (correct) transcription\n",
    "reference = \"I am going to the shop to get some bannocks\"\n",
    "\n",
    "# Hypothesis (transcription generated by your model)\n",
    "hypothesis = \"I am going to shop to get some bannock\"\n",
    "\n",
    "# Calculate WER\n",
    "error = wer(reference, hypothesis)\n",
    "\n",
    "print(f\"Word Error Rate: {error:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc11933",
   "metadata": {},
   "source": [
    "The above means my model is 80% accurate, and 20% of the words are incorrect. This might suggest we want to add more Shetland words to the dataset to ensure the model picks them up with accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcfa17",
   "metadata": {},
   "source": [
    "<H2>Value</h2>\n",
    "\n",
    "<p>The introduction of this piece of technology aligns with the goals of Scotland’s AI strategy (2021). The design and purposes of ‘Spikkbot’ strongly align with the following principles:\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li><b> Community and Individual Benefits:</b>\n",
    "‘Spikkbot’ supports this by primarily by empowering Shetland communities to preserve and promote their endangered dialect. Communities and individuals will reap the benefits from a technology that publicises and encourages the use of it for people within and out with the islands.</li>\n",
    "<li><b> Building Trust and Transparency with AI:</b>\n",
    "‘Spikkbot’ promotes trust and transparency of AI technologies by incorporating human input. As locals are encouraged to contribute to the training of the LLM, it will help to ensure accuracy of the bot.</li>\n",
    "<li><b> Inclusive Digital Growth:</b>\n",
    "The platform aims to support inclusive digital growth by providing tools that are particularly use for rural users in Shetland, along with setting an example for other rural communities. Due to the aspect of voice-based interactions, it is accessible for users with varying levels of digital literacy. Similarly, the typing aspect also helps those with speech difficulties. The application will encourage intergenerational learning and participation, which is important as the dialect is declining in younger populations.</li>\n",
    "<li><b> Community-Led Initiatives:</b>\n",
    "This application aims to create a place-based digital innovation which is grounded in local cultural and heritage. Additionally, it might be the first introduction of AI to small communities, meaning it could be a key educational tool.</li>\n",
    "<li><b> Educational Benefit</b>\n",
    "‘Spikkbot’ aims to educate people about the Shetland dialect, and to create an interactive and engaging experience that will encourage people to use it in their daily lives, whether that be through daily speech or through phrases and idioms.</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46082412",
   "metadata": {},
   "source": [
    "<h2>Ethics</h2>\n",
    "\n",
    "<p>Addressing the ethics of creating a project like ‘Spikkbot’, the HCD framework was referenced. In line with steps three and six, potential harms of the project include perpetuating dialect stereotypes, possible bias, and a lack of human oversight. \n",
    "\n",
    "This project aims to overlook these by including humans at every stage in the development process – through incorporating locals and experts during dataset creation and training, to feedback from the result. Additionally, ‘Spikkbot’ will be rigorously tested with various Shetland voices and dialects, including those of different ages and genders. It is key to not misrepresent the diversity of dialect.\n",
    "\n",
    "Additionally, the project has been designed with a sustainable future in mind. It is important that Shetland dialect is preserved for future generations, and therefore ‘Spikkbot’ has a long-term need. It will be kept updated based on user feedback and the evolution of language.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
